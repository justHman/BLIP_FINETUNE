<div align="center">

# ğŸ–¼ï¸ BLIP Vietnamese Image Captioning

<img src="images\dataset-cover.png" width="700">


*A fine-tuned BLIP model for generating Vietnamese image captions with support for both accented and unaccented text.*

![License](https://img.shields.io/badge/License-BSD--3--Clause-blue.svg)
![Python](https://img.shields.io/badge/Python-3.10+-green.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-1.10+-red.svg)
![BLIP](https://img.shields.io/badge/BLIP-Salesforce-orange.svg)


[Demo](#-demo--results) â€¢ [Installation](#-installation) â€¢ [Training](#ï¸-training) â€¢ [Inference](#-inference)

</div>

---

## ğŸ“‘ Table of Contents

- [ğŸ¯ Demo & Results](#-demo--results)
- [âœ¨ Features](#-features)
- [ğŸš€ Installation](#-installation)
- [ğŸ“Š Dataset Preparation](#-dataset-preparation)
  - [Directory Structure](#directory-structure)
  - [JSON Format](#json-format)
- [âš™ï¸ Model Configuration](#ï¸-model-configuration)
  - [Accented (Vietnamese with Diacritics)](#accented-vietnamese-with-diacritics)
  - [Unaccented (Vietnamese without Diacritics)](#unaccented-vietnamese-without-diacritics)
- [ğŸ’¾ Download Pretrained Weights](#-download-pretrained-weights)
- [ğŸ‹ï¸ Training](#ï¸-training)
  - [Quick Test Mode](#quick-test-mode)
  - [Full Training](#full-training)
  - [Evaluation](#evaluation)
- [ğŸ”® Inference](#-inference)
- [ğŸ“š References](#-references)

---

## ğŸ¯ Demo & Results

### ğŸ“ˆ Training Metrics

Visual representation of model performance during training:

#### Loss Curves
![Training Losses](resutls/losses.png)
*Model loss progression across training epochs*

#### Evaluation Scores
![Evaluation Scores](resutls/scores.png)
*BLEU, ROUGE-L, and CIDEr metrics on validation set*

### ğŸ“Š Quantitative Results

Model evaluation using standard image captioning metrics:

| Dataset | Model | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | ROUGE-L | CIDEr |
|---------|-------|--------|--------|--------|--------|---------|-------|
| UITViC | Accented | 0.623 | 0.451 | 0.342 | 0.285 | 0.512 | 0.789 |
| UITViC | Unaccented | 0.645 | 0.478 | 0.368 | 0.312 | 0.534 | 0.821 |
| KTViC | Accented | 0.598 | 0.432 | 0.321 | 0.267 | 0.498 | 0.756 |
| KTViC | Unaccented | 0.621 | 0.459 | 0.351 | 0.298 | 0.521 | 0.798 |

### ğŸ¨ Qualitative Examples

Sample predictions from the trained model (coming soon - add your inference results here).

---

## âœ¨ Features

- ğŸ‡»ğŸ‡³ **Accented Vietnamese Support** - Native Vietnamese with diacritics using PhoBERT tokenizer
- ğŸ“ **Unaccented Vietnamese Support** - Simplified Vietnamese without diacritics using BERT tokenizer
- ğŸ—‚ï¸ **Multi-Dataset Training** - Fine-tune on UITViC, KTViC, or custom datasets
- ğŸ¨ **Custom Dataset Support** - Easy integration of your own image-caption data
- âš¡ **Quick Test Mode** - Rapid validation with small dataset samples
- ğŸ” **Flexible Inference** - Beam search or nucleus sampling for caption generation
- ğŸ“Š **Comprehensive Evaluation** - BLEU, ROUGE-L, and CIDEr metrics

---

## ğŸš€ Installation

### ğŸ“‹ Prerequisites

- Python 3.10 or higher
- CUDA-compatible GPU (recommended) or CPU
- Git

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/justHman/BLIP_FINETUNE.git
cd BLIP_FINETUNE
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

**ğŸ“¦ Core Dependencies:**
- `torch >= 1.10.0` - PyTorch framework
- `torchvision` - Image transformations
- `timm == 0.4.12` - PyTorch Image Models
- `fairscale == 0.4.4` - PyTorch extensions for high performance and large scale training
- `transformers == 4.26.1` - Hugging Face transformers (PhoBERT, BERT)
- `tokenizers == 0.13.3` - Fast tokenizers for transformers
- `Pillow` - Image processing
- `requests` - HTTP requests
- `pyyaml == 6.0.3` - YAML parser and emitter
- `ruamel.yaml == 0.17.21` - YAML configuration
- `pycocotools` - COCO evaluation tools
- `pycocoevalcap == 1.2` - MS-COCO Caption Evaluation for Python 3
- `tqdm` - Progress bars

### 3ï¸âƒ£ Verify Installation

```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA Available: {torch.cuda.is_available()}')"
```

Expected output:
```
PyTorch: 1.10.0+cu113
CUDA Available: True
```

---

## ğŸ“Š Dataset Preparation

### Directory Structure

Create your custom dataset following this structure in `dataset/custom_dataset/`:

```
BLIP/dataset/custom_dataset/
â”œâ”€â”€ ğŸ“ test_images/
â”‚   â”œâ”€â”€ ğŸ–¼ï¸ 205086.png
â”‚   â”œâ”€â”€ ğŸ–¼ï¸ 304473.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ğŸ“ train_images/
â”‚   â”œâ”€â”€ ğŸ–¼ï¸ 205086.png
â”‚   â”œâ”€â”€ ğŸ–¼ï¸ 304473.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ğŸ“ valid_images/
â”‚   â”œâ”€â”€ ğŸ–¼ï¸ 205086.png
â”‚   â”œâ”€â”€ ğŸ–¼ï¸ 304473.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ğŸ“„ custom_test_gt.json
â”œâ”€â”€ ğŸ“„ custom_valid_gt.json
â”œâ”€â”€ ğŸ“„ test_annnotations.json
â”œâ”€â”€ ğŸ“„ train_annnotations.json
â””â”€â”€ ğŸ“„ valid_annnotations.json
```

### JSON Format

#### 1ï¸âƒ£ Annotation Files (train/valid/test_annnotations.json)

These files contain image information and captions for training:

```json
{
  "images": [
    {
      "id": 205086,
      "filename": "205086.png"
    },
    {
      "id": 304473,
      "filename": "304473.png"
    }
  ],
  "annotations": [
    {
      "id": 0,
      "image_id": 205086,
      "caption": "Ä‘Ã¢y lÃ  khung cáº£nh xuáº¥t hiá»‡n á»Ÿ phÃ­a trÆ°á»›c má»™t cÄƒn nhÃ ",
      "segment_caption": "Ä‘Ã¢y lÃ  khung_cáº£nh xuáº¥t_hiá»‡n á»Ÿ phÃ­a trÆ°á»›c má»™t cÄƒn nhÃ "
    },
    {
      "id": 1,
      "image_id": 304473,
      "caption": "cÃ³ má»™t cÄƒn nhÃ  cao táº§ng xuáº¥t hiá»‡n á»Ÿ trong bá»©c áº£nh",
      "segment_caption": "cÃ³ má»™t cÄƒn nhÃ  cao_táº§ng xuáº¥t_hiá»‡n á»Ÿ trong bá»©c áº£nh"
    }
  ]
}
```

**ğŸ“ Notes:**
- `id` in `images`: Unique image identifier
- `filename`: Image filename (must match files in images folder)
- `caption`: Image description (accented or unaccented depending on configuration)
- `segment_caption`: Word-segmented caption (optional, for PhoBERT)

#### 2ï¸âƒ£ Ground Truth Files (custom_valid_gt.json, custom_test_gt.json)

These files are used for model evaluation (COCO evaluation format):

```json
{
  "info": {
    "description": "custom dataset"
  },
  "images": [
    {
      "id": 205086
    },
    {
      "id": 304473
    }
  ],
  "annotations": [
    {
      "image_id": 205086,
      "caption": "mot cau be dang chong day tren qua banh",
      "id": 7288
    },
    {
      "image_id": 304473,
      "caption": "mot cau be dang chong dau len qua bong da hit dat",
      "id": 3395
    }
  ]
}
```

**ğŸ“ Notes:**
- Ground truth files are only needed for validation and test sets
- Captions in these files are references for comparing predictions
- Multiple captions can share the same `image_id`

---

## âš™ï¸ Model Configuration

Create a YAML configuration file in the `configs/` directory. There are two main configuration types:

### Accented (Vietnamese with Diacritics)

**File:** `configs/accented_custom.yaml`

```yaml
# Dataset paths
root: 'dataset/custom_dataset'
train_image_dir: 'dataset/custom_dataset/train_images'
train_ann: 'dataset/custom_dataset/train_annnotations.json'
valid_image_dir: 'dataset/custom_dataset/valid_images'
valid_ann: 'dataset/custom_dataset/valid_annnotations.json'
test_image_dir: 'dataset/custom_dataset/test_images'
test_ann: 'dataset/custom_dataset/test_annnotations.json'

# Model configuration
image_size: 384
vit: 'base'
vit_grad_ckpt: True
vit_ckpt_layer: 0
batch_size: 8
init_lr: 1e-5
min_lr: 0
max_epoch: 20
weight_decay: 0.05

# ğŸ”¤ Tokenizer (PhoBERT for Vietnamese with diacritics)
tokenizer: 'vinai/phobert-base'

# ğŸ’¬ Prompt
prompt: 'má»™t bá»©c áº£nh vá» '

# ğŸ’¾ Pretrained model (leave empty for training from scratch)
pretrained: ''

# ğŸ² Sampling
sample: False  # False = beam search, True = nucleus sampling
```

### Unaccented (Vietnamese without Diacritics)

**File:** `configs/unaccented_custom.yaml`

```yaml
# Dataset paths
root: 'dataset/custom_dataset'
train_image_dir: 'dataset/custom_dataset/train_images'
train_ann: 'dataset/custom_dataset/train_annnotations.json'
valid_image_dir: 'dataset/custom_dataset/valid_images'
valid_ann: 'dataset/custom_dataset/valid_annnotations.json'
test_image_dir: 'dataset/custom_dataset/test_images'
test_ann: 'dataset/custom_dataset/test_annnotations.json'

# Model configuration
image_size: 384
vit: 'base'
vit_grad_ckpt: True
vit_ckpt_layer: 0
batch_size: 8
init_lr: 1e-5
min_lr: 0
max_epoch: 20
weight_decay: 0.05

# ğŸ”¤ Tokenizer (BERT for Vietnamese without diacritics)
tokenizer: 'bert-base-uncased'

# ğŸ’¬ Prompt
prompt: 'mot buc anh ve '

# ğŸ’¾ Pretrained model (using pre-trained base model)
pretrained: 'weights/model_base_capfilt_large.pth'

# ğŸ² Sampling
sample: False  # False = beam search, True = nucleus sampling
```

### ğŸ” Key Differences

| Feature | ğŸ‡»ğŸ‡³ Accented | ğŸ“ Unaccented |
|---------|----------|------------|
| **Tokenizer** | `vinai/phobert-base` | `bert-base-uncased` |
| **Pretrained** | None (train from scratch) | `model_base_capfilt_large.pth` |
| **Prompt** | `má»™t bá»©c áº£nh vá» ` | `mot buc anh ve ` |
| **Caption** | Vietnamese with diacritics | Vietnamese without diacritics |

---

## ğŸ’¾ Download Pretrained Weights

### ğŸ“¥ Download from Google Drive

**ğŸ”— Link**: [Google Drive - BLIP Weights](https://drive.google.com/drive/folders/1I5VvQczi0psaxvnqxml-sCyOCF4NstSu?usp=sharing)

#### Available Models

| Model File | Type | Dataset | Epochs | Description |
|------------|------|---------|--------|-------------|
| `accented_uitvic_20e.pth` | ğŸ‡»ğŸ‡³ Accented | UITViC | 20 | Vietnamese with diacritics |
| `unaccented_uitvic_20e.pth` | ğŸ“ Unaccented | UITViC | 20 | Vietnamese without diacritics |
| `accented_ktvic_20e.pth` | ğŸ‡»ğŸ‡³ Accented | KTViC | 20 | Vietnamese with diacritics |
| `unaccented_ktvic_20e.pth` | ğŸ“ Unaccented | KTViC | 20 | Vietnamese without diacritics |

### ğŸ“‚ Setup Weights Directory

After downloading, create a `weights` directory and place the files:

```bash
mkdir weights
# Copy downloaded .pth files into the weights/ directory
```

**Expected directory structure:**

```
BLIP/weights/
â”œâ”€â”€ ğŸ“¦ model_base_capfilt_large.pth
â”œâ”€â”€ ğŸ‡»ğŸ‡³ accented_uitvic_20e.pth
â”œâ”€â”€ ğŸ“ unaccented_uitvic_20e.pth
â”œâ”€â”€ ğŸ‡»ğŸ‡³ accented_ktvic_20e.pth
â””â”€â”€ ğŸ“ unaccented_ktvic_20e.pth
```

---

## ğŸ‹ï¸ Training

### Quick Test Mode

Quickly validate the training pipeline with a small dataset (10 samples):

```bash
python train_caption.py ^
    --dataset custom ^
    --config configs/accented_custom.yaml ^
    --output_dir ./output/test_experiment ^
    --device cuda ^
    --quick_test
```

**âš™ï¸ Parameters:**
- `--dataset custom`: Dataset name (custom/uitvic/ktvic)
- `--config`: Path to YAML configuration file
- `--output_dir`: Directory to save results
- `--device`: Device to use (cuda/cpu)
- `--quick_test`: Quick test mode (only 10 samples, 2 epochs)

**âš¡ Quick test will:**
- Use only 10 training samples
- Train for 2 epochs
- Set batch size = 2
- Very fast for code validation

### Full Training

#### 1ï¸âƒ£ Train with Accented Vietnamese

```bash
python train_caption.py ^
    --dataset custom ^
    --config configs/accented_custom.yaml ^
    --output_dir ./output/accented_custom ^
    --device cuda
```

#### 2ï¸âƒ£ Train with Unaccented Vietnamese

```bash
python train_caption.py ^
    --dataset custom ^
    --config configs/unaccented_custom.yaml ^
    --output_dir ./output/unaccented_custom ^
    --device cuda
```

#### 3ï¸âƒ£ Train on CPU (if no GPU available)

```bash
python train_caption.py ^
    --dataset custom ^
    --config configs/accented_custom.yaml ^
    --output_dir ./output/accented_custom ^
    --device cpu
```

### Evaluation

Evaluate a trained model on the test set without training:

```bash
python train_caption.py ^
    --dataset custom ^
    --config configs/accented_custom.yaml ^
    --output_dir ./output/accented_custom ^
    --device cuda ^
    --evaluate
```

**ğŸ“Š The evaluation will:**
- Load the best checkpoint from `output_dir`
- Run inference on test set
- Compute BLEU, ROUGE-L, and CIDEr metrics
- Save results to `output_dir/result/`

### ğŸ“ Training Output

After training, files will be saved in `output_dir`:

```
output/your_experiment/
â”œâ”€â”€ ğŸ“„ config.yaml              # Configuration used
â”œâ”€â”€ ğŸ“„ log.txt                  # Training logs
â”œâ”€â”€ ğŸ’¾ checkpoint_best.pth      # Best model checkpoint
â””â”€â”€ ğŸ“ result/
    â”œâ”€â”€ ğŸ“Š val_epoch0.json      # Validation results epoch 0
    â”œâ”€â”€ ğŸ“Š val_epoch1.json
    â”œâ”€â”€ ğŸ“Š test_epoch0.json     # Test results epoch 0
    â””â”€â”€ ğŸ“Š test_epoch1.json
```

---

## ğŸ”® Inference

### Run Inference on an Image

#### 1ï¸âƒ£ With Beam Search (Default)

```bash
python inference.py ^
    --image_path "path/to/your/image.jpg" ^
    --model_path "output/your_experiment/checkpoint_best.pth" ^
    --device cuda
```

#### 2ï¸âƒ£ With Nucleus Sampling

```bash
python inference.py ^
    --image_path "path/to/your/image.jpg" ^
    --model_path "output/your_experiment/checkpoint_best.pth" ^
    --device cuda ^
    --sample
```

### ğŸ›ï¸ Inference Parameters

| Parameter | Description | Default |
|---------|-------|----------|
| `--image_path` | Path to image for captioning | None (uses demo image) |
| `--model_path` | Path to model checkpoint | **Required** |
| `--device` | Device (cuda/cpu) | cuda (if available) |
| `--sample` | Use nucleus sampling instead of beam search | False |

### ğŸ’¡ Examples

```bash
# Inference with pretrained model
python inference.py ^
    --image_path "dataset/custom_dataset/test_images/205086.png" ^
    --model_path "weights/accented_uitvic_20e.pth" ^
    --device cuda

# Inference with newly trained model
python inference.py ^
    --image_path "my_image.jpg" ^
    --model_path "output/accented_custom/checkpoint_best.pth" ^
    --device cuda ^
    --sample
```

---

## ğŸ“š References

### ğŸ“„ Papers

- **BLIP**: [BLIP: Bootstrapping Language-Image Pre-training](https://arxiv.org/abs/2201.12086)
- **PhoBERT**: [PhoBERT: Pre-trained language models for Vietnamese](https://arxiv.org/abs/2003.00744)

### ğŸ—‚ï¸ Datasets

- **UITViC**: [UIT-ViIC: A Dataset for Vietnamese Image Captioning](https://arxiv.org/abs/2002.00175)
- **KTViC**: Vietnamese Image Captioning Dataset

### ğŸ’» Code

- **Original BLIP**: [salesforce/BLIP](https://github.com/salesforce/BLIP)
- **PhoBERT**: [VinAI Research - PhoBERT](https://github.com/VinAIResearch/PhoBERT)

---

## ğŸ“œ License

BSD-3-Clause License

## ğŸ“– Citation

If you use this code, please cite:

```bibtex
@misc{blip-vietnamese,
  author = {Ngo Hoai Nam},
  title = {BLIP Vietnamese Image Captioning},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/justHman/BLIP_FINETUNE}
}
```

## ğŸ“§ Contact

- **GitHub**: [@justHman](https://github.com/justHman)
- **Repository**: [BLIP_FINETUNE](https://github.com/justHman/BLIP_FINETUNE)

---

<div align="center">

**Happy Training! ğŸš€**

Made with â¤ï¸ for Vietnamese Image Captioning

</div>
